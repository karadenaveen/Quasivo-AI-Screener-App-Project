{
    "job_description": "JD\r\nQuasivo AI Challenge \u2013Web App POC (48\u00a0hours)\r\n Goal\r\nCreate a small, locally\u2011hosted web application that uses Google\u2019s Gemini\u00a0API to automate first\u2011round candidate screening for Quasivo.\r\nUse any Copilots for this assignment, we recommend to use open source copilot called CURSOR (https://www.cursor.com/downloads) \r\n\r\nWhat the App Must\u00a0Do\r\n1.Collect Inputs\r\n\u2022 Job description\u00a0\u2013 paste text or upload a file.\r\n\u2022 Candidate r\u00e9sum\u00e9\u00a0\u2013 paste text or upload a PDF.\r\n2.Leverage the Gemini\u00a0API\r\n\u2022 Compare the r\u00e9sum\u00e9 with the job description.\r\n\u2022 Generate three custom interview questions.\r\n\u2022 Let the candidate submit text answers.\r\n\u2022 Score each answer on a 1\u00a0\u2013\u00a010 scale.\r\n3.Present Results\r\n\u2022 Show the three questions, answers, and scores in a single summary view.\r\n4.Persist Locally\r\n\u2022 Save every JD, r\u00e9sum\u00e9, questions, answers, and scores as JSON\u202f/\u202ftext files inside `` (no DB or cloud storage).\r\n\r\nTechnical Requirements\r\nRequirement\tDetails\r\nRuntime\tMust run entirely on a local machine (Streamlit, Flask, FastAPI, React\u00a0+\u00a0Express, etc.).\r\nGemini\u00a0API\tMandatory for all generation and evaluation steps.\r\nStorage\tLocal folder only (``).\r\nNo cloud deploy\tThe evaluator will run the project locally.\r\n\r\nAPI Details\r\nAPI_GENERATIVE_LANGUAGE_CLIENT=AIzaSyCQTFhomyBU-eLnOt9HsnBq3Ro0Wy9xQCY\r\nconst GEMINI_API_URL =\r\n  `https://generativelanguage.googleapis.com/v1beta/models/` +\r\n  `gemini-2.0-flash:generateContent?key=${GEMINI_API_KEY}`;\r\n\uf0b7Use any HTTP client (axios, fetch, Google\u00a0SDK, etc.).\r\n\uf0b7Do\u00a0not commit this key to a public repo \u2013 load it from an .env file and add that file to .gitignore.\r\n\r\nBonus Ideas (optional)\r\n\uf0b7PDF parsing \u2013 auto\u2011extract text from uploaded r\u00e9sum\u00e9s.\r\n\uf0b7Local SQL\u00a0Server storage \u2013 persist data in a local DB instead of flat files.\r\n\uf0b7Voice input \u2013 capture spoken answers and transcribe them.\r\n\uf0b7Static login \u2013 simple reviewer authentication before viewing results.\r\n\r\nWhat to Submit (within 48\u00a0hours)\r\n1.Project folder (zipped or GitHub/GitLab link)\r\n2.Gemini prompt examples (place in /prompts/)\r\n3.README explaining:\r\n\u2022 How to run the app locally (prereqs, install, start)\r\n\u2022 Key libraries\u00a0/ frameworks used\r\n\u2022 Folder structure and where data are stored\r\n\r\nEvaluation Snapshot\r\n\uf0b7Functional completeness (all required steps work)\r\n\uf0b7Code clarity and documentation\r\n\uf0b7Simplicity and usability of the UI\r\n\uf0b7Secure, correct use of the Gemini\u00a0API\r\n\uf0b7Creativity on any bonus items\r\n\r\nGood luck \u2014 we\u2019re excited to see your ideas!",
    "resume": "NAVEENKUMAR KARADE\nNASSCOM Certified Data Scientist /Top 8 percentile in Kaggle Competition\n\ue316Hyderabad, Telangana.\ue3b89822511501\ue0acnaveenkumarkarade1999@gmail.com\ue2e2Click Here Portfolio\nNaveen Karade Naveen Karade\nSummary\nData Scientist & ML and AI Enthusiast with a strong academic background in machine learning, artificial\nintelligence, and data analysis. Proficient in Python, SQL, and ML & AI frameworks such as TensorFlow and scikit\nlearn. Eager and Algorithm Models to apply theoretical knowledge to real-world data challenges.\nEducation\nMarathwada Institute of Technology, Aurangabad.\nB.Tech With 7.93 CGPA - 2024\nExperience\nCODTECH IT SOLUTIONS\nData Science Intern - FEB 2025 To Apr 2025\nHyderabad, Telangana.\nUmason\u2019s Auto Compo Pvt.Ltd. Aurangabad.\nGET - june 2024 - Aug 2024\nAurangabad, Maharashtra.\nData Scientist\nReal-Time Projects\nExperience in 10+ real-time data science and analytics projects across Various domains such as banking,\nhealthcare, and e-commerce, integrating tools like Python, ML, AI, Agentic AI, LLM, SQL, NLP, Gen-AI and Power BI\nto address real-world problems.\nLLM-Powered EDA Automation\nBuilt an AI-driven tool using LLMs (GPT-4 + PandasAI) to automate exploratory data analysis\u2014instantly generating\ninsights, visualizations, and summaries from raw datasets. Enabled faster, smarter data profiling with a Streamlit,\nGradio UI and reduced manual EDA effort by 70%.\nTech Stack: Python, Pandas, PandasAI, GPT-4, Streamlit, Seaborn, Gradio.\nTechnical Skill's\nPython MYSQL Statistical Modeling\nData Mining Optimization Data Analytics\nExploratory Data Analysis (EDA) Data Cleaning Data Wrangling\nMachine Learning Supervised Learning( Regression,\nClassification)Unsupervised Learning (Clustering)\nFeature Engineering Artificial Intelligence NLP, LLM, BERT\nComputer Vision GenerativeA-I , Agentic AI, Ollama. Deep Learning\nNeural Networks (CNNs, RNNs,\nANNs)Frameworks: TensorFlow, Keras Langchain , Transformers\nProjects\nAI-Powered Chatbot using LLM (Mistral Model)\nLLM, Langchain, RAG, NLP, Embedding\nDeveloped an AI chatbot leveraging LLM-based Generative AI to analyze and tag free-text cancer medical records.\nBuilt a taxonomy-based classification system to categorize symptoms, diagnoses, treatments, and outcomes.\nEnhanced NLP model performance to improve clinical decision-making and oncology research insights.\nTechnologies: LLM (Mistral), Python, NLP, Generative AI, Prompt Engineering\nMedicine Recommendation System :\nML, NLP, LSTM, BERT, GEN AI\nDesigned a machine learning system to provide personalized medicine recommendations based on symptoms,\nmedical history, and prescriptions. Ensured patient safety by minimizing risks like drug interactions and allergies.\nTechnologies: Scikit-learn, TensorFlow, ML, AI, Healthcare Analytics\nTelcom Customer Churn :\nANN, Classification Model\nBuilt and deployed ML models (Decision Trees, ANN) to predict customer churn with 84% accuracy for a telecom\nprovider. Enabled targeted retention strategies through feature engineering and model optimization.\nTools & Skills: Python, Deep Learning, Pandas, NumPy, Scikit-learn, ANN, Model Evaluation\nIBM HR Salary Prediction Project :\nML - Regression Model\nDeveloped predictive models using Linear Regression, Random Forest, and Gradient Boosting to forecast salaries\nbased on demographics, experience, and job roles. Executed data cleaning, feature engineering, and visualization.\nTools: Python, Pandas, Scikit-learn, SQL, Matplotlib, Seaborn\nLicense Plate Recognition System (CV + OCR) :\nComputer Vision\nEngineered a real-time License Plate Recognition system using CNNs and Tesseract OCR for number plate\nextraction from images/videos. Applied in surveillance, parking, and traffic analytics with high accuracy.\nTechnologies: Python, OpenCV, TensorFlow, Keras, Tesseract, Computer Vision\nCertifications\nFull Stack Data Science And AI\nNaresh-IT Hyderabad\nGoogle Advanced Data Analytics Capstone\nGoogle (Coursera)\n25 Aug\nNASSCOM Certified Data Scientist\nLanguages\nEnglish Hindi Marathi Telugu\nAchievements\nKaggle Competitions\nRanked in the top 8 Percentile in the Kaggle Titanic Machine Learning Competition\n\ue2e2https://www.kaggle.com/nsk1234",
    "questions": [
        "1.  Describe a situation where you had to optimize a prompt for the Gemini API to achieve a specific output or improve the quality of the generated content. What techniques did you use?",
        "2.  Explain your approach to local data persistence for the web application. What considerations did you make when choosing JSON/text files over a local SQL database, given the potential scalability and querying benefits of a database?",
        "3.  Given the project's emphasis on running locally and not committing the API key, describe the steps you would take to ensure the security of the Gemini API key and prevent it from being exposed during development and deployment (locally)."
    ],
    "answers": [
        "I used the Gemini API to generate interview questions based on resumes and job descriptions. Initially, the output was too generic, so I optimized the prompt by adding clear role-specific instructions, using few-shot examples, and explicitly asking the model to focus on hands-on skills from the JD. This improved the quality and relevance of the generated questions significantly.",
        "For the initial version of the web application, I chose local JSON/text files for data persistence due to their simplicity, lightweight setup, and quick read/write without the overhead of database configuration. This approach was ideal for a prototype where:\n\nThe dataset was small and not performance-intensive\n\nSchema flexibility was needed (easy to store nested objects like model responses)\n\nI wanted fast iteration without managing DB connections or migrations",
        "No"
    ],
    "scores": [
        "8",
        "9",
        "1"
    ]
}